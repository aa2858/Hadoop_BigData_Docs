
file mover - move files from ftp to shared folders

file 		- load file to vertica raw tables 	

file grouping - sql server jobs that generate event when full file set comes (3 maseters data and 1 fact file), grouping tranformation group all data together 

file tranformation -- sequence of transformation done at retailer level, differnt sequence for different retailers , final table prepared after applying all transformation, each transformation result us one new table



az account list --query '[].{Subscription_ID:id,Tenant_ID:tenantId,Name:name}'  --output table

Subscription_ID                       Tenant_ID                             Name
------------------------------------  ------------------------------------  --------------------------------
d9bc7a93-5c4c-4004-9474-9d11f24faa8b  229363b9-643d-4073-9e9e-2119173d5975  2017 Microsoft Azure Sponsorship

Register your client application with Azure AD

we dont have access to add new client application  to Active directory 
we need app_id and password to get authentication token which is require to call any Azure REST API 

curl -X "POST" "https://login.microsoftonline.com/$TENANTID/oauth2/token" \
-H "Cookie: flight-uxoptin=true; stsservicecookie=ests; x-ms-gateway-slice=productionb; stsservicecookie=ests" \
-H "Content-Type: application/x-www-form-urlencoded" \
--data-urlencode "client_id=$APPID" \
--data-urlencode "grant_type=client_credentials" \
--data-urlencode "client_secret=$PASSWORD" \
--data-urlencode "resource=https://management.azure.com/"


 put request 
 https://management.azure.com/subscriptions/d9bc7a93-5c4c-4004-9474-9d11f24faa8b/resourceGroups/spark/providers/Microsoft.HDInsight/clusters/pocSparkClusterShalaj?api-version=2015-03-01-preview
 
 Alt+f12 to open terminal in intellij
 
kafka-topics.bat --delete --zookeeper localhost:2181 --topic topic1
kafka-topics.bat --list --zookeeper localhost:2181




vertx run com.vertx.kafka.KafkaProducerVerticle -cp Vertex-fat.jar;cluster.xml -cluster -cluster-host 72.27.147.51

java -Dvertx.hazelcast.config=D:/ShalajS/sw/vert.x-3.4.2/vertx/conf/cluster.xml -jar Vertex-fat.jar

vertx run com.vertx.kafka.MySqlVerticle -cp Vertex-fat.jar -cluster -cluster-host 172.27.155.92
vertx run com.vertx.kafka.KafkaConsumerVerticle -cp Vertex-fat.jar -cluster -cluster-host 72.27.147.51 -cluster-port 35678
vertx run com.vertx.kafka.KafkaProducerVerticle -cp Vertex-fat.jar -cluster -cluster-host 72.27.147.51 -cluster-port 35678


vertx run com.acme.MyVerticle -cp "classes:lib/myjar.jar" -instances 10

vertx run com.vertx.kafka.MySqlVerticle -cp Vertex-fat.jar -cluster -cluster-host 172.27.147.51

vertx start com.vertx.kafka.KafkaProducerVerticle -cp Vertex-fat.jar -cluster -cluster-host 72.27.147.51
vertx start com.vertx.kafka.KafkaConsumerVerticle -cp Vertex-fat.jar -cluster -cluster-host 72.27.147.51

vertx run org.rest.RESTVerticle -cp rest-0.0.1-SNAPSHOT-fat.jar;conf -cluster -cluster-host 172.27.147.51

vertx run org.producer.KafkaProducerVerticle -cp producer-0.0.1-SNAPSHOT-fat.jar;cluster.xml -cluster -cluster-host 172.27.147.51

we need to create separate pom file for all 
java -jar pong-1.0-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51
java -jar ping-1.0-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.155.92

two way to run the verticles
cd D:\ShalajS\java_projects\VertxPoc\target

1. vertx run com.vertx.kafka.RESTVerticle -cp Vertex-fat.jar -cluster
2. vertx run com.vertx.kafka.MySqlVerticle -cp Vertex-fat.jar -cluster
3. vertx run com.vertx.kafka.KafkaConsumerVerticle -cp Vertex-fat.jar -cluster
4. vertx run com.vertx.kafka.KafkaProducerVerticle -cp Vertex-fat.jar -cluster

cd D:\ShalajS\COE_Analysis\RSI\hazelcast-cluster
java -jar producer-0.0.1-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51 
java -jar rest-0.0.1-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51 -instances 2
java -jar consumer-0.0.1-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51 
java -jar -Dvertx.options.metricsEnabled=true dashboard-0.0.1-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51 

/home/mac92/rsi/hazelcast-cluster
java -jar mysql-0.0.1-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.155.92 


# to run in background
java -jar rest-0.0.1-SNAPSHOT-fat.jar start -Dvertx-id=restVerticle -cp conf -cluster -cluster-host 172.27.147.51 -ha
# to stop application 
java -jar rest-0.0.1-SNAPSHOT-fat.jar stop restVerticle
(it didnt stop it , need random id generated by start command inplace of restVerticle)


java -jar  -Djava.util.logging.config.file=D:\ShalajS\java_projects\Test\src\mylogging.properties rest-0.0.1-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51 -instances 2

java -jar rest-0.0.1-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.155.92 -ha


// install maven on centos
wget http://www-eu.apache.org/dist/maven/maven-3/3.5.0/binaries/apache-maven-3.5.0-bin.tar.gz
sudo tar xzf apache-maven-3.5.0-bin.tar.gz -C /usr/local
cd /usr/local
sudo ln -s apache-maven-3.5.0 maven
 
Next, set up Maven path system-wide:

$ sudo vi /etc/profile.d/maven.sh
export M2_HOME=/usr/local/maven
export PATH=${M2_HOME}/bin:${PATH}

Finally, log out and log in again to activate the above environment variables.
To verify successful installation of maven, check the version of maven:

$mvn -version


 CREATE TABLE metrics1 (
  emp_id varchar(100),
  start_time timestamp,
  end_time timestamp,
  process_time DECIMAL(5,1)
);

 CREATE TABLE metrics2 (
  emp_id varchar(100),
  start_time BIGINT,
  end_time BIGINT,
  process_time DECIMAL(5,1),
  counter varchar(100)
)



 gradle tasks   >> shows us all the task that are currently available
 
 java -jar producer-1.0-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51 
 java -jar rest-1.0-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51 -instances 2
java -jar consumer-1.0-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51 
java -jar -Dvertx.options.metricsEnabled=true dashboard-1.0-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51 
java -jar mysql-1.0-SNAPSHOT-fat.jar -cp conf -cluster -cluster-host 172.27.147.51  


CREATE TABLE `etl_config` (
  `etl_id` varchar(10) NOT NULL,
  `retailer_id` varchar(20) DEFAULT NULL,
  `etl_name` varchar(20) DEFAULT NULL,
  `etl_config` varchar(500) DEFAULT NULL,
  `start_type` varchar(100) DEFAULT NULL,
  `schedule_time` time DEFAULT NULL,
  `parent_id` varchar(10) DEFAULT NULL,
  `etl_state` varchar(10) DEFAULT NULL,
  PRIMARY KEY (`etl_id`)
);

INSERT INTO etl_config VALUES ('001','Ahold','fileCopy','{"configtype":"filecopy","filepattern":"adhold*.dat","etl_id":"001","address":"fileCopyEventProcessor"}','scheduled','2:30',null,'Active');
INSERT INTO etl_config VALUES ('002','Ahold','sparkSubmit1','{"configtype":"sparkSubmit","inputParam":"a b c","etl_id":"002","address":"sparkSubmitEventProcessor"}','chained',null,'001','Active');
INSERT INTO etl_config VALUES ('003','Ahold','oneMore','{"configtype":"sparkSubmit2","etl_id":"003","address":"sparkSubmitEventProcessor"}','chained',null,'001','Active');
INSERT INTO etl_config VALUES ('004','Ahold','final','{"configtype":"final","etl_id":"004","address":"sparkSubmitEventProcessor"}','chained',null,'003','Active');
INSERT INTO etl_config VALUES ('005','Walgreen','fileCopy','{"configtype":"filecopy","filepattern":"walgreen*.dat","etl_id":"005","address":"fileCopyEventProcessor"}','scheduled','2:30',null,'Active');
INSERT INTO etl_config VALUES ('006','walgreen','sparkSubmit','{"configtype":"sparkSubmit","inputParam":"a b c","etl_id":"006","address":"sparkSubmitEventProcessor"}','chained',null,'005','Active');


CREATE TABLE `etl_logs` (
  `etl_id` varchar(10) DEFAULT NULL,
  `process_dt_time` datetime DEFAULT NULL,
  `status` varchar(50) DEFAULT NULL
);

update etl_logs set process_dt =CURDATE()-1;

update etl_config set schedule_time = CURRENT_TIME where start_type ='scheduled'

SELECT distinct cf.etl_id,retailer_id,etl_name,etl_config FROM etl_config cf LEFT JOIN etl_log lg ON cf.etl_id=lg.etl_id WHERE start_type='scheduled' AND etl_state ='Active' AND schedule_time < CURRENT_TIME() AND (lg.last_process_dt < CURDATE() OR last_process_dt IS NULL) OR lg.status='failure';


SELECT distinct cf.etl_id,retailer_id,etl_name,etl_config FROM etl_config cf left join (select etl_id,max(process_dt_time) process_dt_time from etl_logs group by etl_id) log on cf.etl_id=log.etl_id WHERE start_type='scheduled' AND etl_state ='Active' AND schedule_time < CURRENT_TIME() AND (log.process_dt_time < CURDATE() OR process_dt_time IS NULL);

SELECT etl_id,retailer_id,etl_name,etl_config FROM etl_config where parent_id='001';
